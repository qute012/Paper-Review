## Self-Distillation for Improving CTC-Transformer-based ASR Systems
Interspeech 2020에 개제된 논문이다.

# Abstract

Sequence-to-sequence(S2S) 모델은 안정적으로 ASR 시스템에 정착되었고, 핵심 요소는 attention 메커니즘이 입력과 아웃풋인 캐릭터를 정렬(alignment)하는 것이다. 어텐션의 가중치는 입력과 출력의 시퀀스의 타임 프레임을 알려준다. 본 연구에서는 매 시점의 트랜스포머의 출력과 어텐션의 가중치를 사용하여 pseudo-targets을 생성하고,  CTC-Transformer의 shared encoder를 Transformer-decoder를 통해 피드백 받아 더 많은 정보를 학습한다. 증류 기법은 흔하지만, 이를 디코더로 부터 피드백을 받는 방법에 대해서는 감이 안와서 method를 더 세밀하게 분석할 필요가 있어보인다. 이런 방식으로 모델을 학습하면 S2S 역시 공유 인코더로 인해서 성능이 강화된다고 한다.

# ASR Modules

