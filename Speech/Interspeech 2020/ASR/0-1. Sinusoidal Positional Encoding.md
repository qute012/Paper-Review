## Sinusoidal Positional Encoding
 https://skyjwoo.tistory.com/entry/positional-encoding에 설명이 잘되어있어서 개인적으로 Bert의 PE pytorch implement와 연관지어 공부해보았다.

인코딩은 네 가지 기준을 충족해야한다고함

1) 각 time-step(문장에서 단어의 위치)마다 하나의 유일한 encoding 값을 출력해 내야 한다. 

2) 서로 다른 길이의 문장에 있어서 두 time-step 간 거리는 일정해야 한다.

3) 모델에 대한 일반화가 가능해야 한다. 더 긴 길이의 문장이 나왔을 때 적용될 수 있어야 한다. 즉, 순서를 나타내는 값 들이 특정 범위 내에 있어야 한다.

 4) 하나의 key 값처럼 결정되어야 한다. 매번 다른 값이 나와선 안된다.



![image](https://user-images.githubusercontent.com/33983084/103995964-86bb2a00-51dc-11eb-9262-4c1640780933.png)



```
class PositionalEmbedding(nn.Module):

    def __init__(self, d_model, max_len=512):
        super().__init__()

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model).float()
        pe.require_grad = False

        position = torch.arange(0, max_len).float().unsqueeze(1)
        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return self.pe[:, :x.size(1)]
```



출처: https://skyjwoo.tistory.com/entry/positional-encoding이란-무엇인가 [jeongstudy]